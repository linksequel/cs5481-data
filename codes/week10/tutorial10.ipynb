{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5481 Data Engineering: Tutorial on Representation Learning and Multimodal Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome everyone to Tutorial 10. Today we're focusing on two key concepts in deep learning: representation learning and multimodal learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Representation Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.08MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.26MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.57MB/s]\n",
      "/opt/anaconda3/envs/cityu/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cityu/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Training on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1875/1875 [05:22<00:00,  5.81it/s, loss=0.416, acc=86.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4165, Accuracy: 86.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:   8%|▊         | 156/1875 [00:26<04:57,  5.79it/s, loss=0.305, acc=90.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    116\u001b[0m evaluate(model, test_loader, device)\n",
      "Cell \u001b[0;32mIn[3], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 80\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to match ResNet50 input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Convert grayscale to RGB\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the model using ResNet50 for feature extraction and a fully connected layer for classification\n",
    "class ResNet50Classifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet50Classifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet50 and remove the classification layer\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        # Freeze the parameters of ResNet50\n",
    "        for param in self.resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Remove the final fully connected layer of ResNet50\n",
    "        self.features = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
    "        \n",
    "        # Add a fully connected layer for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using ResNet50\n",
    "        features = self.features(x)\n",
    "        # Classify using the fully connected layer\n",
    "        output = self.fc(features)\n",
    "        return output\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model = ResNet50Classifier(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': running_loss / (progress_bar.n + 1),\n",
    "                'acc': 100 * correct / total\n",
    "            })\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100*correct/total:.2f}%')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Train the model\n",
    "print(f\"Training on {device}\")\n",
    "train(model, train_loader, criterion, optimizer, device, epochs=5)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader, device)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'resnet50_mnist_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multimodal Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb207a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cityu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/adityajn105/flickr8k?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.04G/1.04G [00:40<00:00, 27.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/sequel/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d339a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size: 3447\n",
      "Dataset contains 6000 images\n",
      "Dataset contains 1000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cityu/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/cityu/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/3], Step [0/375], Loss: 8.1407\n",
      "Epoch [1/3], Step [10/375], Loss: 5.3371\n",
      "Epoch [1/3], Step [20/375], Loss: 4.4625\n",
      "Epoch [1/3], Step [30/375], Loss: 4.1557\n",
      "Epoch [1/3], Step [40/375], Loss: 3.9190\n",
      "Epoch [1/3], Step [50/375], Loss: 3.9802\n",
      "Epoch [1/3], Step [60/375], Loss: 4.0253\n",
      "Epoch [1/3], Step [70/375], Loss: 4.0815\n",
      "Epoch [1/3], Step [80/375], Loss: 3.7740\n",
      "Epoch [1/3], Step [90/375], Loss: 4.0683\n",
      "Epoch [1/3], Step [100/375], Loss: 4.4051\n",
      "Epoch [1/3], Step [110/375], Loss: 3.6088\n",
      "Epoch [1/3], Step [120/375], Loss: 3.6760\n",
      "Epoch [1/3], Step [130/375], Loss: 3.3971\n",
      "Epoch [1/3], Step [140/375], Loss: 3.2381\n",
      "Epoch [1/3], Step [150/375], Loss: 3.8310\n",
      "Epoch [1/3], Step [160/375], Loss: 3.5206\n",
      "Epoch [1/3], Step [170/375], Loss: 3.5303\n",
      "Epoch [1/3], Step [180/375], Loss: 3.5821\n",
      "Epoch [1/3], Step [190/375], Loss: 3.4264\n",
      "Epoch [1/3], Step [200/375], Loss: 3.2605\n",
      "Epoch [1/3], Step [210/375], Loss: 3.6083\n",
      "Epoch [1/3], Step [220/375], Loss: 3.5448\n",
      "Epoch [1/3], Step [230/375], Loss: 3.8234\n",
      "Epoch [1/3], Step [240/375], Loss: 3.2432\n",
      "Epoch [1/3], Step [250/375], Loss: 2.9880\n",
      "Epoch [1/3], Step [260/375], Loss: 3.4296\n",
      "Epoch [1/3], Step [270/375], Loss: 3.5771\n",
      "Epoch [1/3], Step [280/375], Loss: 2.9867\n",
      "Epoch [1/3], Step [290/375], Loss: 3.3757\n",
      "Epoch [1/3], Step [300/375], Loss: 3.4910\n",
      "Epoch [1/3], Step [310/375], Loss: 3.6020\n",
      "Epoch [1/3], Step [320/375], Loss: 3.7012\n",
      "Epoch [1/3], Step [330/375], Loss: 3.4483\n",
      "Epoch [1/3], Step [340/375], Loss: 3.1717\n",
      "Epoch [1/3], Step [350/375], Loss: 3.3091\n",
      "Epoch [1/3], Step [360/375], Loss: 3.0787\n",
      "Epoch [1/3], Step [370/375], Loss: 3.3281\n",
      "Generated caption 1: a man in a red shirt and a black and white dog is jumping over a ball .\n",
      "Generated caption 2: a man in a red shirt and a black and white dog is jumping over a ball .\n",
      "Generated caption 3: a man in a red shirt and a black and white dog is jumping over a ball .\n",
      "Epoch [2/3], Step [0/375], Loss: 3.3476\n",
      "Epoch [2/3], Step [10/375], Loss: 3.3379\n",
      "Epoch [2/3], Step [20/375], Loss: 3.6266\n",
      "Epoch [2/3], Step [30/375], Loss: 3.2812\n",
      "Epoch [2/3], Step [40/375], Loss: 3.5477\n",
      "Epoch [2/3], Step [50/375], Loss: 2.9998\n",
      "Epoch [2/3], Step [60/375], Loss: 3.3624\n",
      "Epoch [2/3], Step [70/375], Loss: 2.9451\n",
      "Epoch [2/3], Step [80/375], Loss: 2.9117\n",
      "Epoch [2/3], Step [90/375], Loss: 3.3514\n",
      "Epoch [2/3], Step [100/375], Loss: 2.8063\n",
      "Epoch [2/3], Step [110/375], Loss: 3.0753\n",
      "Epoch [2/3], Step [120/375], Loss: 3.2096\n",
      "Epoch [2/3], Step [130/375], Loss: 3.0616\n",
      "Epoch [2/3], Step [140/375], Loss: 2.8698\n",
      "Epoch [2/3], Step [150/375], Loss: 3.0906\n",
      "Epoch [2/3], Step [160/375], Loss: 3.6237\n",
      "Epoch [2/3], Step [170/375], Loss: 2.8344\n",
      "Epoch [2/3], Step [180/375], Loss: 2.6112\n",
      "Epoch [2/3], Step [190/375], Loss: 3.5608\n",
      "Epoch [2/3], Step [200/375], Loss: 3.0302\n",
      "Epoch [2/3], Step [210/375], Loss: 3.4629\n",
      "Epoch [2/3], Step [220/375], Loss: 2.7031\n",
      "Epoch [2/3], Step [230/375], Loss: 3.4164\n",
      "Epoch [2/3], Step [240/375], Loss: 3.1055\n",
      "Epoch [2/3], Step [250/375], Loss: 3.2548\n",
      "Epoch [2/3], Step [260/375], Loss: 2.7551\n",
      "Epoch [2/3], Step [270/375], Loss: 2.8696\n",
      "Epoch [2/3], Step [280/375], Loss: 2.8225\n",
      "Epoch [2/3], Step [290/375], Loss: 3.0729\n",
      "Epoch [2/3], Step [300/375], Loss: 3.3414\n",
      "Epoch [2/3], Step [310/375], Loss: 2.8290\n",
      "Epoch [2/3], Step [320/375], Loss: 2.9183\n",
      "Epoch [2/3], Step [330/375], Loss: 3.0237\n",
      "Epoch [2/3], Step [340/375], Loss: 2.9417\n",
      "Epoch [2/3], Step [350/375], Loss: 2.6226\n",
      "Epoch [2/3], Step [360/375], Loss: 3.1506\n",
      "Epoch [2/3], Step [370/375], Loss: 3.2815\n",
      "Generated caption 1: a man is standing in front of a building .\n",
      "Generated caption 2: a man is standing in front of a building .\n",
      "Generated caption 3: a man is standing in front of a building .\n",
      "Epoch [3/3], Step [0/375], Loss: 3.1422\n",
      "Epoch [3/3], Step [10/375], Loss: 3.0664\n",
      "Epoch [3/3], Step [20/375], Loss: 3.1001\n",
      "Epoch [3/3], Step [30/375], Loss: 2.9509\n",
      "Epoch [3/3], Step [40/375], Loss: 2.8711\n",
      "Epoch [3/3], Step [50/375], Loss: 3.0736\n",
      "Epoch [3/3], Step [60/375], Loss: 2.4741\n",
      "Epoch [3/3], Step [70/375], Loss: 2.9598\n",
      "Epoch [3/3], Step [80/375], Loss: 3.2164\n",
      "Epoch [3/3], Step [90/375], Loss: 2.8051\n",
      "Epoch [3/3], Step [100/375], Loss: 2.8961\n",
      "Epoch [3/3], Step [110/375], Loss: 3.0519\n",
      "Epoch [3/3], Step [120/375], Loss: 2.8917\n",
      "Epoch [3/3], Step [130/375], Loss: 3.1500\n",
      "Epoch [3/3], Step [140/375], Loss: 2.9591\n",
      "Epoch [3/3], Step [150/375], Loss: 2.7846\n",
      "Epoch [3/3], Step [160/375], Loss: 2.6366\n",
      "Epoch [3/3], Step [170/375], Loss: 2.9594\n",
      "Epoch [3/3], Step [180/375], Loss: 3.0163\n",
      "Epoch [3/3], Step [190/375], Loss: 2.8337\n",
      "Epoch [3/3], Step [200/375], Loss: 2.9363\n",
      "Epoch [3/3], Step [210/375], Loss: 2.6382\n",
      "Epoch [3/3], Step [220/375], Loss: 3.1100\n",
      "Epoch [3/3], Step [230/375], Loss: 2.7775\n",
      "Epoch [3/3], Step [240/375], Loss: 2.7450\n",
      "Epoch [3/3], Step [250/375], Loss: 2.8408\n",
      "Epoch [3/3], Step [260/375], Loss: 2.9224\n",
      "Epoch [3/3], Step [270/375], Loss: 2.6393\n",
      "Epoch [3/3], Step [280/375], Loss: 2.9674\n",
      "Epoch [3/3], Step [290/375], Loss: 2.7494\n",
      "Epoch [3/3], Step [300/375], Loss: 3.2457\n",
      "Epoch [3/3], Step [310/375], Loss: 3.1539\n",
      "Epoch [3/3], Step [320/375], Loss: 2.7096\n",
      "Epoch [3/3], Step [330/375], Loss: 3.0208\n",
      "Epoch [3/3], Step [340/375], Loss: 2.5509\n",
      "Epoch [3/3], Step [350/375], Loss: 2.6944\n",
      "Epoch [3/3], Step [360/375], Loss: 3.0811\n",
      "Epoch [3/3], Step [370/375], Loss: 3.1107\n",
      "Generated caption 1: a man in a white shirt and black pants is standing on a sidewalk .\n",
      "Generated caption 2: a man in a white shirt and black pants is standing on a sidewalk .\n",
      "Generated caption 3: a man in a white shirt and black pants is standing on a sidewalk .\n",
      "Models saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Configuration parameters\n",
    "vocab_threshold = 4\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "\n",
    "# Simple vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n",
    "        self.idx2word = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n",
    "        self.idx = 4\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __call__(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Simple tokenization function - using space splitting and punctuation handling\n",
    "def simple_tokenize(text):\n",
    "    # Treat punctuation as separate tokens\n",
    "    for punct in ',.!?;:':\n",
    "        text = text.replace(punct, f' {punct} ')\n",
    "    # Split by spaces\n",
    "    return [token.lower() for token in text.split() if token]\n",
    "\n",
    "# Build simple vocabulary\n",
    "def build_vocab(token_file):\n",
    "    vocab = Vocabulary()\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Read caption file\n",
    "    with open(token_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                caption = parts[1].lower()\n",
    "                tokens = simple_tokenize(caption)\n",
    "                for token in tokens:\n",
    "                    if token not in word_counts:\n",
    "                        word_counts[token] = 0\n",
    "                    word_counts[token] += 1\n",
    "    \n",
    "    # Add words that appear more than threshold times\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= vocab_threshold:\n",
    "            vocab.add_word(word)\n",
    "    \n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "# Flickr8k dataset class\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, img_dir, caption_file, img_list_file, vocab, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Read image list\n",
    "        with open(img_list_file, 'r') as f:\n",
    "            self.img_list = [line.strip() for line in f]\n",
    "        \n",
    "        # Read and map captions\n",
    "        self.captions = {}\n",
    "        with open(caption_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    img_id = parts[0].split('#')[0]\n",
    "                    caption = parts[1].lower()\n",
    "                    if img_id in self.img_list:\n",
    "                        if img_id not in self.captions:\n",
    "                            self.captions[img_id] = []\n",
    "                        self.captions[img_id].append(caption)\n",
    "        \n",
    "        # Keep only images with captions\n",
    "        self.img_list = [img for img in self.img_list if img in self.captions]\n",
    "        print(f\"Dataset contains {len(self.img_list)} images\")\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_list[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.img_dir, img_id)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Randomly select one caption\n",
    "        caption = np.random.choice(self.captions[img_id])\n",
    "        \n",
    "        # Tokenize and convert to indices\n",
    "        tokens = simple_tokenize(caption)\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<start>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('<end>'))\n",
    "        \n",
    "        return image, torch.Tensor(caption).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "# Simplified collate function for DataLoader\n",
    "def collate_fn(data):\n",
    "    images, captions = zip(*data)\n",
    "    \n",
    "    # Get lengths and create padded tensor\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    \n",
    "    # Padding\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    \n",
    "    # Stack images\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    return images, targets, lengths\n",
    "\n",
    "# Image encoder (ResNet50 simplified)\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # Use pretrained ResNet50\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the final fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        return features\n",
    "\n",
    "# Text decoder (simplified, avoiding pack_padded_sequence)\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        # Embed all caption words\n",
    "        embeddings = self.embed(captions)\n",
    "        \n",
    "        # Add image features as first input\n",
    "        batch_size = features.size(0)\n",
    "        features = features.unsqueeze(1)\n",
    "        embeddings = torch.cat((features, embeddings[:, :-1]), 1)\n",
    "        \n",
    "        # LSTM forward pass (without pack_padded_sequence)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        \n",
    "        # Predict next word\n",
    "        outputs = self.linear(hiddens)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, max_len=20):\n",
    "        \"\"\"Generate captions (inference mode)\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        states = None\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens.squeeze(1))\n",
    "            _, predicted = outputs.max(1)\n",
    "            sampled_ids.append(predicted)\n",
    "            \n",
    "            # Stop condition\n",
    "            if (predicted == 2).sum() == predicted.size(0): # All samples reached <end>\n",
    "                break\n",
    "                \n",
    "            # Input for next time step\n",
    "            inputs = self.embed(predicted).unsqueeze(1)\n",
    "        \n",
    "        return torch.stack(sampled_ids, 1)\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define paths\n",
    "    data_dir = '/Users/sequel/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1'\n",
    "    img_dir = os.path.join(data_dir, 'images')\n",
    "    token_file = os.path.join(data_dir, 'Flickr8k.token.txt')\n",
    "    train_file = os.path.join(data_dir, 'Flickr_8k.trainImages.txt')\n",
    "    val_file = os.path.join(data_dir, 'Flickr_8k.devImages.txt')\n",
    "    \n",
    "    # Image transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = build_vocab(token_file)\n",
    "    \n",
    "    # Create datasets and data loaders\n",
    "    train_dataset = Flickr8kDataset(img_dir, token_file, train_file, vocab, transform)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    val_dataset = Flickr8kDataset(img_dir, token_file, val_file, vocab, transform)\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize models\n",
    "    encoder = EncoderCNN(embedding_dim).to(device)\n",
    "    decoder = DecoderRNN(embedding_dim, hidden_dim, len(vocab)).to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx['<pad>'])\n",
    "    params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            \n",
    "            # Calculate loss - using reshape to match dimensions\n",
    "            outputs = outputs.reshape(-1, outputs.shape[2])\n",
    "            targets = captions.reshape(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Show progress\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Generate sample captions\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get a batch of validation data\n",
    "            images, _, _ = next(iter(val_loader))\n",
    "            images = images[:3].to(device)  # Use only 3 images as examples\n",
    "            \n",
    "            # Generate captions\n",
    "            features = encoder(images)\n",
    "            sampled_ids = decoder.sample(features)\n",
    "            \n",
    "            # Convert indices to words\n",
    "            for i in range(len(sampled_ids)):\n",
    "                sampled_caption = []\n",
    "                for word_id in sampled_ids[i].cpu().numpy():\n",
    "                    word = vocab.idx2word.get(word_id, '<unk>')\n",
    "                    if word == '<end>':\n",
    "                        break\n",
    "                    if word not in ['<start>', '<pad>']:\n",
    "                        sampled_caption.append(word)\n",
    "                \n",
    "                print(f\"Generated caption {i+1}: {' '.join(sampled_caption)}\")\n",
    "    \n",
    "    # Save models\n",
    "    torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "    torch.save(decoder.state_dict(), 'decoder.pth')\n",
    "    print(\"Models saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cityu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
